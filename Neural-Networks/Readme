logistic回归算法{
    1.拟合函数
    2.代价函数
    3.梯度下降算法
    4.logistic回归算法
}

Neural Networks
{
    1.拟合函数
    2.代价函数
    3.梯度下降
    4.神经网络
}

//Neural Networks
//graph composistion
//g(x) = 1/(1+e^(-x))
// inlayer(0)  hiden-layer(1)  outlayer(2)
// a01          a11             a21 
// a02          a12             a22
// a03          a13             a23
//              a14
//类似邻接表
//a01 -> a10 = k<0>11 ----k<n>ij :n为layer i为该层的节点索引 j为下一层的节点索引
//a01 -> a11 = k<0>12
//a01 -> a12 = k<0>13
//a02 -> a10 = k<0>21
//...
//设定a<n,m>(n为layer，m为单层的节点索引) = g(z)
//what the "z": eg->   z<1,1> = a01*k<0>11 + a02*k<0>21 + a03*k<0>31
//即该节点的所有入度的加权和
//so that a<n,m> = g( ∑[ (a<n-1,i>) * (k<n-1><i,m>) ] )
//eg: a12 = g( ∑[ a01*k<0>12 + a02*k<0>22 + a03*k<0>23 ] )
//向量表示：Van = [an1 , an2 , an3 , ... , ans]^T
//上述公式对inlayer除外(不计算第一层--输入层)
//这样我们就能通过给定的输入得到每一层任何一个节点的具体值
//cost function
//back-propagation-algorithm
//设定δ<n,m>(同样除掉第一层-=输入没有误差):为第n层第m个节点的误差值
//eg:   δ21 = a21 - yi<1> --- yi<n> i代表训练特征对应的结果的索引 n在多元分类时有意义，代表本索引结果中对应的元素
//      δ22 = a22 - yi<2>
//      δ23 = a23 - yi<3>
//向量表示：Vδn = [δn1 , δn2 ,δn3 ,..., δns]^T
//And:  δ1 = Vk<1> *  Vδ2 * g'(Vz<1>)
//        __                                             __         
//Vk<n> = | k<n><1,1>  k<n><1,2>  k<n><1,3> ... k<n><1,s> |
//        | k<n><2,1>  k<n><2,2>  k<n><2,3> ... k<n><2,s> | 
//        | k<n><3,1>  k<n><3,2>  k<n><3,3> ... k<n><3,s> | 
//        |   .            .          .     ...           |
//        |   .            .          .     ...           |
//        |   .            .          .     ...           |
//        |_k<n><t,1>       ... ... ...     ... k<n><3,s>_|
//so  
//        __                                 __
//Vk<1> = |  k<n><1,1>  k<n><1,2>  k<n><1,3>  |
//        |  k<n><2,1>  k<n><2,2>  k<n><2,3>  | 
//        |  k<n><3,1>  k<n><3,2>  k<n><3,3>  |
//        |_ k<n><4,1>  k<n><4,2>  k<n><4,3> _| 
//                                          
//Vδ2 =   [ δ21 , δ22 , δ23 ]^T
//g'(Vz<n>) = Van * (1-Van)
//        __                                                              __
//Vz<n> = |  a<n-1,1>*k<n-1>11 + a<n-1,2>*k<n-1>21 +...+ a<n-1,s>*k<n-1>s1 |
//        |  a<n-1,1>*k<n-1>12 + a<n-1,2>*k<n-1>22 +...+ a<n-1,s>*k<n-1>s2 |
//        |  .                         .                        .          |
//        |  .                         .                        .          |
//        |  .                         .                        .          |
//        |_ a<n-1,1>*k<n-1>1t + a<n-1,2>*k<n-1>2t +...+ a<n-1,s>*k<n-1>st_|
//=>g'(Vz<n>) = Van * (1-Van)
//证： g(x) = 1/(1+e^(-x)) => g'(x) = e^(-x)/(1+e^(-x))^2
//取向量中的一个元素
//eg: g'(z<1,1>) = e^( -z<1,1> )/( 1+e^(-z<1,1>) )^2
//    又 a<1,1> = g(z<1,1>) = 1/(1+e^(-z<1,1>)) 
//     1-a<1,1> = 1- g(z<1,1>) = e^(-z<1,1>)/(1+e^(-z<1,1>)) 
//a<1,1> * (1- a<1,1>) equal to g'(z<1,1>)
//所以 g'(Vz<n>) = Van * (1-Van)  
//综上
//for hiden-layer 
//δ<n,m> = ∑(k<n>mi * δ2i) * a<n,m> * (1-a<n,m>)
//Vδn =  Vkn * δ(n+1) * Van * (1-Van)    